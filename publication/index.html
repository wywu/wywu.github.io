<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="googlea477729bc8866235"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publication | Wayne Wu</title> <meta name="author" content="Wayne Wu"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/milky-way.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://wywu.github.io/publication/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://wywu.github.io/">Wayne Wu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publication/">Publication<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/project/">Industry</a> </li> <li class="nav-item "> <a class="nav-link" href="/dataset/">Dataset</a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">Software</a> </li> <li class="nav-item "> <a class="nav-link" href="/Others/">Others</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publication</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/he2025seeing.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="he2025seeing" class="col-sm-7"> <div class="title">From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning</div> <div class="author">Honglin He, Yukai Ma,  <em>Wayne Wu</em>, and Bolei Zhou </div> <div class="periodical"> <em>Technical report, arXiv:2507.22028,</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2507.22028" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://metadriverse.github.io/s2e/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/wayne2025urbansim.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="wayne2025urbansim" class="col-sm-7"> <div class="title">Towards Autonomous Micromobility through Scalable Urban Simulation</div> <div class="author"> <em>Wayne Wu</em> *, Honglin He*, Chaoyuan Zhang, Jack He, Seth Z. Zhao, Ran Gong, Quanyi Li, and Bolei Zhou </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition (CVPR),</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2505.00690" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://metadriverse.github.io/urban-sim/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Highlight</span> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/ziyang2025vid2sim.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="ziyang2025vid2sim" class="col-sm-7"> <div class="title">Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation</div> <div class="author">Ziyang Xie, Zhizheng Liu, Zhenghao Peng,  <em>Wayne Wu</em>, and Bolei Zhou </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition (CVPR),</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2501.06693" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://metadriverse.github.io/vid2sim/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/zhizheng2024josh.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="zhizheng2024josh" class="col-sm-7"> <div class="title">Joint Optimization for 4D Human-Scene Reconstruction in the Wild</div> <div class="author">Zhizheng Liu, Joe Lin,  <em>Wayne Wu</em>, and Bolei Zhou </div> <div class="periodical"> <em>Technical report, arXiv:2501.02158,</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2501.02158" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://genforce.github.io/JOSH/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/zhizheng2025pedgen.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="zhizheng2025pedgen" class="col-sm-7"> <div class="title">Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels</div> <div class="author">Zhizheng Liu, Joe Lin,  <em>Wayne Wu</em>, and Bolei Zhou </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR),</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2410.07500" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://genforce.github.io/PedGen/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/wayne2025metaurban.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="wayne2025metaurban" class="col-sm-7"> <div class="title">MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility</div> <div class="author"> <em>Wayne Wu</em> *, Honglin He*, Jack He, Yiran Wang, Chenda Duan, Zhizheng Liu, Quanyi Li, and Bolei Zhou </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR),</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2407.08725" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=vHuAzNxmfKc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://metadriverse.github.io/metaurban/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Spotlight</span> </div> </div> </li> </ol> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/baixin2024parameterization.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="baixin2024parameterization" class="col-sm-7"> <div class="title">Parameterization-driven Neural Surface Reconstruction for Object-oriented Editing in Neural Rendering</div> <div class="author">Baixin Xu, Jiangbei Hu, Fei Hou, Kwan-Yee Lin,  <em>Wayne Wu</em>, Chen Qian, and Ying He </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2024 </div> <div class="links"> <a href="http://arxiv.org/abs/2310.05524" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://xubaixinxbx.github.io/neuparam/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/shikai2024cosmicman.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="shikai2024cosmicman" class="col-sm-7"> <div class="title">CosmicMan: A Text-to-Image Foundation Model for Humans</div> <div class="author">Shikai Li, Jianglin Fu, Kaiyuan Liu, Wentao Wang, Kwan-Yee Lin, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition (CVPR),</em> 2024 </div> <div class="links"> <a href="http://arxiv.org/abs/2404.01294" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=fk2fniU6oyM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://cosmicman-cvpr2024.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Highlight</span> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/jianhui2024painthuman-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/jianhui2024painthuman-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/jianhui2024painthuman-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/jianhui2024painthuman.png"> </picture> </figure> </div> <div id="jianhui2024painthuman" class="col-sm-7"> <div class="title">PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via Denoised Score Distillation</div> <div class="author">Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>Association for the Advancement of Artificial Intelligence (AAAI),</em> 2024 </div> <div class="links"> <a href="http://arxiv.org/abs/2310.09458" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/haonan2024relitalk-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/haonan2024relitalk-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/haonan2024relitalk-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/haonan2024relitalk.png"> </picture> </figure> </div> <div id="haonan2024relitalk" class="col-sm-7"> <div class="title">ReliTalk: Relightable Talking Portrait Generation from a Single Video</div> <div class="author">Haonan Qiu, Zhaoxi Chen, Yuming Jiang, Hang Zhou, Xiangyu Fan, Lei Yang,  <em>Wayne Wu</em>, and Ziwei Liu </div> <div class="periodical"> <em>International Journal of Computer Vision (IJCV),</em> 2024 </div> <div class="links"> <a href="http://arxiv.org/abs/2309.02434" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=tS2Tek_72J0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="http://haonanqiu.com/projects/ReliTalk.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/jintao2024vlg-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/jintao2024vlg-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/jintao2024vlg-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/jintao2024vlg.png"> </picture> </figure> </div> <div id="jintao2024vlg" class="col-sm-7"> <div class="title">VLG: General Video Recognition with Web Textual Knowledge</div> <div class="author">Jintao Lin, Zhaoyang Liu, Wenhai Wang,  <em>Wayne Wu</em>, and Limin Wang </div> <div class="periodical"> <em>International Journal of Computer Vision (IJCV),</em> 2024 </div> <div class="links"> <a href="http://arxiv.org/abs/2212.01638" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhuo2023hyperstyle3d-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhuo2023hyperstyle3d-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhuo2023hyperstyle3d-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhuo2023hyperstyle3d.png"> </picture> </figure> </div> <div id="zhuo2023hyperstyle3d" class="col-sm-7"> <div class="title">HyperStyle3D: Text-Guided 3D Portrait Stylization via Hypernetworks</div> <div class="author">Zhuo Chen, Xudong Xu,  et al.,  <em>Wayne Wu</em>, Bo Dai, and Xiaokang Yang </div> <div class="periodical"> <em>Technical report, arXiv:2304.09463,</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2304.09463" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/contributors2023renderme360.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="contributors2023renderme360" class="col-sm-7"> <div class="title">RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars</div> <div class="author"> Contributors to RenderMe-360 </div> <div class="periodical"> <em>Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks,</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2305.13353" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=nIgrtQwkrdg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://renderme-360.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/contributors2023dnarendering.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="contributors2023dnarendering" class="col-sm-7"> <div class="title">DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering</div> <div class="author"> Contributors to DNA-Rendering </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2307.10173" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=xlhfvxvu7nc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://dna-rendering.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/zhitao2023synbody.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="zhitao2023synbody" class="col-sm-7"> <div class="title">SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling</div> <div class="author">Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai,  <em>Wayne Wu</em>, Chen Qian, Dahua Lin, Ziwei Liu, and Lei Yang </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2303.17368" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=ogXpRB9zR9A" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://synbody.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/jianglin2023unitedhuman.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="jianglin2023unitedhuman" class="col-sm-7"> <div class="title">UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation</div> <div class="author">Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Ziwei Liu, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="https://www.youtube.com/watch?v=pdsfUYFDLSw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://unitedhuman.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/honglin2023orthoplanes.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="honglin2023orthoplanes" class="col-sm-7"> <div class="title">OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs</div> <div class="author">Honglin He, Zhuoqian Yang, Shikai Li, Bo Dai, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="https://www.youtube.com/watch?v=hHts2zWEbJ8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://orthoplanes.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/zhuoqian2022hg3d.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="zhuoqian2022hg3d" class="col-sm-7"> <div class="title">3DHumanGAN: Towards Photo-Realistic 3D-Aware Human Image Generation</div> <div class="author">Zhuoqian Yang, Shikai Li,  <em>Wayne Wu</em>†, and Bo Dai </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2212.07378" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=-bUNfhNYj24" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://3dhumangan.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/wentao2022motionbert.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="wentao2022motionbert" class="col-sm-7"> <div class="title">MotionBERT: Unified Pretraining for Human Motion Analysis</div> <div class="author">Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu,  <em>Wayne Wu</em>, and Yizhou Wang </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2210.06551" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://motionbert.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/yuming2023text2performer.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="yuming2023text2performer" class="col-sm-7"> <div class="title">Text2Performer: Text-Driven Human Video Generation</div> <div class="author">Yuming Jiang, Shuai Yang, Tong Liang Koh,  <em>Wayne Wu</em>, Chen Change Loy, and Ziwei Liu </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2304.08483" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=YwhaJUk_qo0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://yumingj.github.io/projects/Text2Performer.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/yuxin2023learning.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="yuxin2023learning" class="col-sm-7"> <div class="title">Learning Unified Decompositional and Compositional NeRF for Editable Novel View Synthesis</div> <div class="author">Yuxin Wang,  <em>Wayne Wu</em>, and Dan Xu </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2308.02840" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://w-ted.github.io/publications/udc-nerf/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/zhengming2023monohuman.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="zhengming2023monohuman" class="col-sm-7"> <div class="title">MonoHuman: Animatable Human Neural Field from Monocular Video</div> <div class="author">Zhengming Yu, Wei Cheng, Xian Liu,  <em>Wayne Wu</em>, and Kwan-Yee Lin </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2304.02001" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://yzmblog.github.io/projects/MonoHuman/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/jianhui2023celebvtext.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="jianhui2023celebvtext" class="col-sm-7"> <div class="title">CelebV-Text: A Large-Scale Facial Text-Video Dataset</div> <div class="author">Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2303.14717" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://celebv-text.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/tong2023omniobject3d-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/tong2023omniobject3d-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/tong2023omniobject3d-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/tong2023omniobject3d.png"> </picture> </figure> </div> <div id="tong2023omniobject3d" class="col-sm-7"> <div class="title">OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation</div> <div class="author">Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan,  <em>Wayne Wu</em>, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, and Ziwei Liu </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2301.07525" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://omniobject3d.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Best Paper Candidate</span> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/haoyue2023filter-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/haoyue2023filter-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/haoyue2023filter-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/haoyue2023filter.png"> </picture> </figure> </div> <div id="haoyue2023filter" class="col-sm-7"> <div class="title">Filter-Recovery Network for Multi-Speaker Audio-Visual Speech Separation</div> <div class="author">Haoyue Cheng, Zhaoyang Liu,  <em>Wayne Wu</em>, and Limin Wang </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR),</em> 2023 </div> <div class="links"> <a href="https://openreview.net/forum?id=fiB2RjmgwQ6" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/wei2022gnr.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="wei2022gnr" class="col-sm-7"> <div class="title">Generalizable Neural Performer: Learning Robust Radiance Fields for Human Novel View Synthesis</div> <div class="author">Wei Cheng, Su Xu, Jingtan Piao, Chen Qian,  <em>Wayne Wu</em>, Kwan-Yee Lin, and Hongsheng Li </div> <div class="periodical"> <em>Technical report, arXiv:2204.11798,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2204.11798" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=2COR4u1ZIuk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://generalizable-neural-performer.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/haonan2022stylefacev.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="haonan2022stylefacev" class="col-sm-7"> <div class="title">StyleFaceV: Face Video Generation via Decomposing and Recomposing Pretrained StyleGAN3</div> <div class="author">Haonan Qiu, Yuming Jiang, Hang Zhou,  <em>Wayne Wu</em>, and Ziwei Liu </div> <div class="periodical"> <em>Technical report, arXiv:2208.07862,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2208.07862" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=BZNLcD04-Fc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="http://haonanqiu.com/projects/StyleFaceV.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/xian2022audio.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="xian2022audio" class="col-sm-7"> <div class="title">Audio-Driven Co-Speech Gesture Video Generation</div> <div class="author">Xian Liu, Qianyi Wu, Hang Zhou, Yuanqi Du,  <em>Wayne Wu</em>, Dahua Lin, and Ziwei Liu </div> <div class="periodical"> <em>Neural Information Processing Systems (NeurIPS),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2212.02350" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://alvinliu0.github.io/projects/ANGIE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Spotlight</span> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/long2022fastv2v.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="long2022fastv2v" class="col-sm-7"> <div class="title">Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis</div> <div class="author">Long Zhuo, Guangcong Wang, Shikai Li,  <em>Wayne Wu</em>, and Ziwei Liu </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2207.05049" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=AhEqjGVuk4A" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://fast-vid2vid.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/jianglin2022styleganhuman.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="jianglin2022styleganhuman" class="col-sm-7"> <div class="title">StyleGAN-Human: A Data-Centric Odyssey of Human Generation</div> <div class="author">Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy,  <em>Wayne Wu</em>†, and Ziwei Liu </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2205.15996" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=nIrb9hwsdcI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://stylegan-human.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/xian2022semanticaware-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/xian2022semanticaware-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/xian2022semanticaware-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/xian2022semanticaware.png"> </picture> </figure> </div> <div id="xian2022semanticaware" class="col-sm-7"> <div class="title">Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation</div> <div class="author">Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou,  <em>Wayne Wu</em>, and Bolei Zhou </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2201.07786" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://alvinliu0.github.io/projects/SSP-NeRF" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Oral</span> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/hao2022celebvhq.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="hao2022celebvhq" class="col-sm-7"> <div class="title">CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</div> <div class="author">Hao Zhu*,  <em>Wayne Wu</em> *†, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2207.12393" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=Y0uxlUW4sW0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://celebv-hq.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/haoyue2022jointmodal-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/haoyue2022jointmodal-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/haoyue2022jointmodal-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/haoyue2022jointmodal.png"> </picture> </figure> </div> <div id="haoyue2022jointmodal" class="col-sm-7"> <div class="title">Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing</div> <div class="author">Haoyue Cheng, Zhaoyang Liu, Hang Zhou, Chen Qian,  <em>Wayne Wu</em>, and Limin Wang </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2204.11573" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/yuming2022text.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="yuming2022text" class="col-sm-7"> <div class="title">Text2Human: Text-Driven Controllable Human Image Generation</div> <div class="author">Yuming Jiang, Shuai Yang, Haonan Qiu,  <em>Wayne Wu</em>, Chen Change Loy, and Ziwei Liu </div> <div class="periodical"> <em>ACM Transaction on Graphics (SIGGRAPH),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2205.15996" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=yKh4VORA_E0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://yumingj.github.io/projects/Text2Human.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/xinya2022eamm.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="xinya2022eamm" class="col-sm-7"> <div class="title">EAMM: One-Shot Emotional Talking Face via Audio-based Emotion-Aware Motion Model</div> <div class="author">Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu,  <em>Wayne Wu</em>†, Feng Xu, and Xun Cao </div> <div class="periodical"> <em>ACM Transaction on Graphics (SIGGRAPH),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2205.15278" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=VT53MzPzhWM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/yanbo2022transeditor-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/yanbo2022transeditor-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/yanbo2022transeditor-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/yanbo2022transeditor.png"> </picture> </figure> </div> <div id="yanbo2022transeditor" class="col-sm-7"> <div class="title">TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing</div> <div class="author">Yanbo Xu, Yueqin Yin, Liming Jiang, Qianyi Wu, Chengyao Zheng, Chen Change Loy, Bo Dai, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2203.17266" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://billyxyb.github.io/TransEditor/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/xian2022learning.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="xian2022learning" class="col-sm-7"> <div class="title">Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</div> <div class="author">Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou,  <em>Wayne Wu</em>, Bo Dai, and Bolei Zhou </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2203.13161" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=CG632W-nIWk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://alvinliu0.github.io/projects/HA2G" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/jiaqi2022progressive-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/jiaqi2022progressive-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/jiaqi2022progressive-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/jiaqi2022progressive.png"> </picture> </figure> </div> <div id="jiaqi2022progressive" class="col-sm-7"> <div class="title">Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection</div> <div class="author">Jiaqi Tang, Zhaoyang Liu, Chen Qian,  <em>Wayne Wu</em>, and Limin Wang </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2112.04771" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/mocanet-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/mocanet-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/mocanet-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/mocanet.png"> </picture> </figure> </div> <div id="wentao2022mocanet" class="col-sm-7"> <div class="title">MoCaNet: Motion Retargeting in-the-wild via Canonicalization Networks</div> <div class="author">Wentao Zhu, Zhuoqian Yang, Ziang Di,  <em>Wayne Wu</em>†, Yizhou Wang, and Chen Change Loy </div> <div class="periodical"> <em>Association for the Advancement of Artificial Intelligence (AAAI),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2112.10082" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://yzhq97.github.io/mocanet/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/linsen2022everybody.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="linsen2022everybody" class="col-sm-7"> <div class="title">Everybody’s Talkin’: Let Me Talk as You Want</div> <div class="author">Linsen Song,  <em>Wayne Wu</em>, Chen Qian, Ran He, and Chen Change Loy </div> <div class="periodical"> <em>Transactions on Information Forensics and Security (TIFS)</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2001.05201" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=tNPuAnvijQk&amp;feature=emb_imp_woyt" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/EBT/EBT.html" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/liming2022deepfake-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/liming2022deepfake-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/liming2022deepfake-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/liming2022deepfake.png"> </picture> </figure> </div> <div id="liming2022deepfake" class="col-sm-7"> <div class="title">DeepFakes Detection: The DeeperForensics Dataset and Challenge</div> <div class="author">Liming Jiang,  <em>Wayne Wu</em>, Chen Qian, and Chen Change Loy </div> <div class="periodical"> <em>Handbook of Digital Face Manipulation and Detection, Springer,</em> 2022 </div> <div class="links"> <a href="https://link.springer.com/book/10.1007/978-3-030-87664-7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/yuxin2022talking-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/yuxin2022talking-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/yuxin2022talking-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/yuxin2022talking.png"> </picture> </figure> </div> <div id="yuxin2022talking" class="col-sm-7"> <div class="title">Talking Faces: Audio-to-Video Face Generation</div> <div class="author">Yuxin Wang, Linsen Song,  <em>Wayne Wu</em>, Chen Qian, Ran He, and Chen Change Loy </div> <div class="periodical"> <em>Handbook of Digital Face Manipulation and Detection, Springer,</em> 2022 </div> <div class="links"> <a href="https://link.springer.com/book/10.1007/978-3-030-87664-7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/liming2021deceive-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/liming2021deceive-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/liming2021deceive-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/liming2021deceive.jpeg"> </picture> </figure> </div> <div id="liming2021deceive" class="col-sm-7"> <div class="title">Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data</div> <div class="author">Liming Jiang, Bo Dai,  <em>Wayne Wu</em>, and Chen Change Loy </div> <div class="periodical"> <em>Neural Information Processing System (NeurIPS),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2111.06849" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=3Luz817WpZM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://www.mmlab-ntu.com/project/apa/index.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/liming2021focal-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/liming2021focal-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/liming2021focal-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/liming2021focal.jpeg"> </picture> </figure> </div> <div id="liming2021focal" class="col-sm-7"> <div class="title">Focal Frequency Loss for Image Reconstruction and Synthesis</div> <div class="author">Liming Jiang, Bo Dai,  <em>Wayne Wu</em>, and Chen Change Loy </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2012.12821" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.mmlab-ntu.com/project/ffl/index.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhaoyang2021tam-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhaoyang2021tam-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhaoyang2021tam-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhaoyang2021tam.png"> </picture> </figure> </div> <div id="zhaoyang2021tam" class="col-sm-7"> <div class="title">TAM: Temporal Adaptive Module for Video Recognition</div> <div class="author">Zhaoyang Liu, Limin Wang,  <em>Wayne Wu</em>, Chen Qian, and Tong Lu </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2005.06803" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/liu-zhy/temporal-adaptive-module" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/linsen2021everything.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="linsen2021everything" class="col-sm-7"> <div class="title">Everything’s Talkin’: Pareidolia Face Reenactment</div> <div class="author">Linsen Song*,  <em>Wayne Wu</em> *, Chaoyou Fu, Chen Qian, Chen Change Loy, and Ran He </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2001.05201" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=yKh4VORA_E0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/ETT/ETT.html" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/xinya2021evp.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="xinya2021evp" class="col-sm-7"> <div class="title">Audio-Driven Emotional Video Portraits</div> <div class="author">Xinya Ji, Hang Zhou, Kaisiyuan Wang,  <em>Wayne Wu</em>†, Chen Change Loy, Xun Cao, and Feng Xu </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2104.07452" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=o6LQfLkizbw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://jixinya.github.io/projects/evp/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/hang2022pose.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="hang2022pose" class="col-sm-7"> <div class="title">Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation</div> <div class="author">Hang Zhou, Yasheng Sun,  <em>Wayne Wu</em>, Chen Change Loy, Xiaogang Wang, and Ziwei Liu </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=lNQQHIggnUg&amp;feature=emb_logo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://hangz-nju-cuhk.github.io/projects/PC-AVS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/hao2020aot-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/hao2020aot-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/hao2020aot-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/hao2020aot.png"> </picture> </figure> </div> <div id="hao2020aot" class="col-sm-7"> <div class="title">AOT: Appearance Optimal Transport Based Identity Swapping for Forgery Detection</div> <div class="author">Hao Zhu, Chaoyou Fu, Qianyi Wu,  <em>Wayne Wu</em>, Chen Qian, and Ran He </div> <div class="periodical"> <em>Neural Information Processing System (NeurIPS),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2011.02674" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/xiaokang2020bidirectional-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/xiaokang2020bidirectional-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/xiaokang2020bidirectional-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/xiaokang2020bidirectional.png"> </picture> </figure> </div> <div id="xiaokang2020bidirectional" class="col-sm-7"> <div class="title">Bi-directional Cross-Modality Feature Propagation with SA Gate for RGB-D Semantic Segmentation</div> <div class="author">Xiaokang Chen, Kwan-Yee Lin, Jingbo Wang,  <em>Wayne Wu</em>, Chen Qian, Hongsheng Li, and Gang Zeng </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2007.09183" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/charlesCXK/RGBD_Semantic_Segmentation_PyTorch" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/kaisiyuan2020mead-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/kaisiyuan2020mead-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/kaisiyuan2020mead-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/kaisiyuan2020mead.png"> </picture> </figure> </div> <div id="kaisiyuan2020mead" class="col-sm-7"> <div class="title">MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation</div> <div class="author">Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang,  <em>Wayne Wu</em>†, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2020 </div> <div class="links"> <a href="" class="btn btn-sm z-depth-0" role="button">YouTube</a> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660698.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://wywu.github.io/projects/MEAD/MEAD.html" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/liming2020deeperforensics-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/liming2020deeperforensics-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/liming2020deeperforensics-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/liming2020deeperforensics.png"> </picture> </figure> </div> <div id="liming2020deeperforensics" class="col-sm-7"> <div class="title">DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection</div> <div class="author">Liming Jiang, Ren Li,  <em>Wayne Wu</em>, Chen Qian, and Chen Change Loy </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2001.03024" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=b6iKqkJht38&amp;feature=emb_imp_woyt" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://liming-jiang.com/projects/DrF1/DrF1.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/zhuoqian2020transmomo.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="zhuoqian2020transmomo" class="col-sm-7"> <div class="title">TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting</div> <div class="author">Zhuoqian Yang*, Wentao Zhu*,  <em>Wayne Wu</em> *, Chen Qian, Qiang Zhou, Bolei Zhou, and Chen Change Loy </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2003.14401" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=akbRtnRMkMk&amp;feature=youtu.be" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://yzhq97.github.io/transmomo/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> <h2 class="year">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wayne2019transgaga-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wayne2019transgaga-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wayne2019transgaga-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wayne2019transgaga.png"> </picture> </figure> </div> <div id="wayne2019transgaga" class="col-sm-7"> <div class="title">TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation</div> <div class="author"> <em>Wayne Wu</em>, Kaidi Cao, Cheng Li, Chen Qian, and Chen Change Loy </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1904.09571" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://wywu.github.io/projects/TGaGa/TGaGa.html" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/keqiang2019fab-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/keqiang2019fab-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/keqiang2019fab-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/keqiang2019fab.png"> </picture> </figure> </div> <div id="keqiang2019fab" class="col-sm-7"> <div class="title">FAB: A Robust Facial Landmark Detection Framework for Motion-Blurred Videos</div> <div class="author">Keqiang Sun,  <em>Wayne Wu</em>, Tinghao Liu, Shuo Yang, Quan Wang, Qiang Zhou, Zuochang Ye, and Chen Qian </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1910.12100" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/KeqiangSun/FAB" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/shengju2019aggregation-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/shengju2019aggregation-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/shengju2019aggregation-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/shengju2019aggregation.png"> </picture> </figure> </div> <div id="shengju2019aggregation" class="col-sm-7"> <div class="title">Aggregation via Separation: Boosting Facial Landmark Detector with Self-Supervised Style Transition</div> <div class="author">Shengju Qian, Keqiang Sun,  <em>Wayne Wu</em>, Chen Qian, and Jiaya Jia </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1908.06440" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/shengju2019make-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/shengju2019make-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/shengju2019make-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/shengju2019make.png"> </picture> </figure> </div> <div id="shengju2019make" class="col-sm-7"> <div class="title">Make a Face: Towards Arbitrary High Fidelity Face Manipulation</div> <div class="author">Shengju Qian, Kwan-Yee Lin,  <em>Wayne Wu</em>, Yangxiaokang Liu, Quan Wang, Fumin Shen, Chen Qian, and Ran He </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1908.07191" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/wayne2018reenactgan.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="wayne2018reenactgan" class="col-sm-7"> <div class="title">ReenactGAN: Learning to Reenact Faces via Boundary Transfer</div> <div class="author"> <em>Wayne Wu</em>, Yunxuan Zhang, Cheng Li, Chen Qian, and Chen Change Loy </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/abs/1807.11079" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=LBAfeKrHMys" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/ReenactGAN/ReenactGAN.html" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/wayne2018look.mp4" class="pub-video" preload="metadata" muted="" playsinline="" autoplay="" loop="" onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="wayne2018look" class="col-sm-7"> <div class="title">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</div> <div class="author"> <em>Wayne Wu</em>, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, and Qiang Zhou </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/abs/1805.10483" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=B5IIOYlL4w0&amp;feature=emb_imp_woyt" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/LAB/LAB.html" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Wayne Wu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: October 04, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-4GRHT7MFBP"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-4GRHT7MFBP");</script> </body> </html>