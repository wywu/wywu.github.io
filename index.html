<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="googlea477729bc8866235"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Wayne Wu</title> <meta name="author" content="Wayne Wu"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/milky-way.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://wywu.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%77%75%77%65%6E%79%61%6E%30%35%30%33@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=uWfZKz4AAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/wywu" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/wayne-wu-042b97166" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/wayne_wu_0503" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publication/">Publication</a> </li> <li class="nav-item "> <a class="nav-link" href="/project/">Industry</a> </li> <li class="nav-item "> <a class="nav-link" href="/dataset/">Dataset</a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">Software</a> </li> <li class="nav-item "> <a class="nav-link" href="/Others/">Others</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Wayne Wu </h1> <p class="desc"></p> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/wayne_wu-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/wayne_wu-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/wayne_wu-1400.webp"></source> <img class="img-fluid z-dept-100 rounded" src="/assets/img/wayne_wu.jpg" alt="wayne_wu.jpg"> </picture> </figure> </div> <div class="clearfix"> <p>I am a Research Associate in the Department of Computer Science at the University of California, Los Angeles, working with <a href="https://boleizhou.github.io/" target="_blank" rel="noopener noreferrer">Bolei Zhou</a>. Previously, I served as a Research Scientist at Shanghai AI Lab, where I led the Virtual Human Group, working with <a href="http://dahua.site/" target="_blank" rel="noopener noreferrer">Dahua Lin</a>. I was also a Visiting PhD at Nanyang Technological University, working with <a href="https://www.mmlab-ntu.com/person/ccloy/" target="_blank" rel="noopener noreferrer">Chen Change Loy</a>. In June 2022, I obtained my PhD in the Department of Computer Science and Technology at Tsinghua University.</p> <div class="research-slogan"> <em>"I build world simulators and models for physical AI; I put people first, always."</em> </div> <div class="research"> <h2>Research</h2> <p>My research lies at the intersection of computer vision, computer graphics, and robotics. I aim to develop <strong style="color: rgb(255, 60, 0);">human-centric physical AI systems</strong> capable of perceiving, understanding, and interacting with real-world environments populated by humans. To advance this vision, I address key challenges in the <em>scalability</em> of robot learning environments, the <em>situational awareness</em> of agents, and the <em>realism</em> of populated virtual humans. My work explores three primary directions:</p> <ul> <li> <strong>Scalable World Simulators</strong>: Developing <u>large-scale robot learning platforms</u> with diverse assets and infinite urban scenes, and enabling high-efficiency robot training, as in <a href="https://metadriverse.github.io/metaurban/" target="_blank" rel="noopener noreferrer">MetaUrban</a>, <a href="publication/">URBAN-SIM</a>, <a href="https://metadriverse.github.io/vid2sim/" target="_blank" rel="noopener noreferrer">Vid2Sim</a>, and <a href="https://omniobject3d.github.io/" target="_blank" rel="noopener noreferrer">OmniObject3D</a>.</li> <li> <strong>Situational Behavior Modeling</strong>: Building <u>autonomous decision-making models</u> of humans and other agents, to behave robustly based on multimodal understanding of surroundings -- including vision, audio, and language, as in <a href="https://embodiedhuman.github.io/" target="_blank" rel="noopener noreferrer">EmbodiedHuman</a>, <a href="https://genforce.github.io/PedGen/" target="_blank" rel="noopener noreferrer">PedGen</a>, and <a href="https://metadriverse.github.io/s2e/" target="_blank" rel="noopener noreferrer">Seeing-to-Experiencing</a>.</li> <li> <strong>Realistic Virtual Humans</strong>: Constructing <u>high-fidelity 4D volumetric capture systems and datasets</u>, as in <a href="https://dna-rendering.github.io/" target="_blank" rel="noopener noreferrer">DNA-Rendering</a> and <a href="https://renderme-360.github.io/" target="_blank" rel="noopener noreferrer">RenderMe-360</a>; and developing <u>human foundation models</u> to obtain generalizable representations, as in <a href="https://cosmicman-cvpr2024.github.io/" target="_blank" rel="noopener noreferrer">CosmicMan</a> and <a href="https://motionbert.github.io/" target="_blank" rel="noopener noreferrer">MotionBert</a>.</li> </ul> </div> <style>.research-slogan{text-align:center;font-size:1.3em;margin:1.5em 0;color:#ff3c00!important;font-weight:bold!important}.research-slogan em{color:#ff3c00!important;font-weight:bold!important}</style> </div> <div class="talks"> <h2>Recent Talks and Lectures</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <td class="date">Oct, 2025</td> <td class="announcement"> Guest lecture, “Building Scalable and Safe Physical AI Systems”, @ <a href="https://www.cis.upenn.edu/" target="_blank" rel="noopener noreferrer">Upenn</a>, hosted by <a href="https://lingjie0206.github.io/" target="_blank" rel="noopener noreferrer">Lingjie Liu</a>. </td> </tr> <tr> <td class="date">Jul, 2025</td> <td class="announcement"> Invited talk, “Building Scalable Physical AI Systems”, @ <a href="https://svl.stanford.edu/" target="_blank" rel="noopener noreferrer">SVL Lab in Stanford</a>, hosted by <a href="https://jiajunwu.com/" target="_blank" rel="noopener noreferrer">Jiajun Wu</a>. </td> </tr> <tr> <td class="date">Jul, 2025</td> <td class="announcement"> Invited talk, “Building Scalable Physical AI Systems”, @ <a href="https://corp.roblox.com/" target="_blank" rel="noopener noreferrer">Roblox</a>, hosted by <a href="https://davidbdurst.com/" target="_blank" rel="noopener noreferrer">David Durst</a>. </td> </tr> <tr> <td class="date">Jun, 2025</td> <td class="announcement"> Invited talk, “Scaling-up Urban Simulation for Autonomous Micro-mobility”, @ <a href="https://real2simworkshop.github.io/" target="_blank" rel="noopener noreferrer">Real-to-Sim Workshop CVPR</a>. </td> </tr> <tr> <td class="date">Jan, 2025</td> <td class="announcement"> Invited talk, “Scaling-up Urban Simulation for Autonomous Micro-mobility”, @ <a href="https://bair.berkeley.edu/" target="_blank" rel="noopener noreferrer">BAIR Lab in UC Berkeley</a>, hosted by <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank" rel="noopener noreferrer">Trevor Darrell</a> and <a href="https://people.eecs.berkeley.edu/~kanazawa/" target="_blank" rel="noopener noreferrer">Angjoo Kanazawa</a>. </td> </tr> <tr> <td class="date">Jun, 2024</td> <td class="announcement"> Invited talk, “Simulation Platforms for Embodied AI in Urban Spaces”, @ <a href="https://poets2024.github.io/" target="_blank" rel="noopener noreferrer">POETS Workshop CVPR</a>. </td> </tr> <tr> <td class="date">Apr, 2024</td> <td class="announcement"> Guest lecture, “Autonomous Agents and Foundation Models”, @ <a href="https://www.cs.ucla.edu/" target="_blank" rel="noopener noreferrer">UCLA</a>, hosted by <a href="https://boleizhou.github.io/" target="_blank" rel="noopener noreferrer">Bolei Zhou</a>. </td> </tr> </table> </div> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <td class="date">May, 2025</td> <td class="announcement"> We are organizing the workshop on <a href="https://real2simworkshop.github.io/" target="_blank" rel="noopener noreferrer">Real-to-Sim: Bridging the Gap between Neural Rendering and Robot Learning</a> at CVPR 2025. 🔥 </td> </tr> <tr> <td class="date">May, 2025</td> <td class="announcement"> We are organizing the workshop on <a href="https://poets2024.github.io/poets2025/" target="_blank" rel="noopener noreferrer">Embodied “Humans”: Symbiotic Intelligence Between Virtual Humans and Humanoid Robots</a> at CVPR 2025. 🔥 </td> </tr> <tr> <td class="date">Jan, 2025</td> <td class="announcement"> We released <a href="https://github.com/metadriverse/metaurban" target="_blank" rel="noopener noreferrer">MetaUrban</a> – a simulation platform for Embodied AI in urban spaces. Try it now! </td> </tr> <tr> <td class="date">Mar, 2024</td> <td class="announcement"> We are organizing the workshop on <a href="https://poets2024.github.io/" target="_blank" rel="noopener noreferrer">Virtual Humans for Robotics and Autonomous Driving</a> at CVPR 2024. </td> </tr> <tr> <td class="date">Jun, 2023</td> <td class="announcement"> <a href="https://omniobject3d.github.io/" target="_blank" rel="noopener noreferrer">OmniObject3D</a> is selected as Best Paper Candidate at CVPR 2023. </td> </tr> <tr> <td class="date">Oct, 2022</td> <td class="announcement"> I am leading <a href="https://openxdlab.org.cn/home" target="_blank" rel="noopener noreferrer">OpenXDLab</a> – a new large-scale open-source data platform! </td> </tr> <tr> <td class="date">Aug, 2020</td> <td class="announcement"> We are organizing <a href="https://sense-human.github.io/index_2020.html" target="_blank" rel="noopener noreferrer">Workshop on Sensing, Understanding and Synthesizing Humans</a>, ECCV 2020. </td> </tr> <tr> <td class="date">Jul, 2020</td> <td class="announcement"> We released <a href="https://github.com/open-mmlab/mmaction2" target="_blank" rel="noopener noreferrer">MMAction2</a> – OpenMMLab’s Next Generation Action Understanding Toolbox. </td> </tr> <tr> <td class="date">Jul, 2020</td> <td class="announcement"> We released <a href="https://github.com/open-mmlab/mmediting" target="_blank" rel="noopener noreferrer">MMEditing</a> – OpenMMLab’s Image and Video Editing Toolbox. </td> </tr> <tr> <td class="date">Oct, 2019</td> <td class="announcement"> We are organizing <a href="https://openaccess.thecvf.com/ICCV2019_workshops/ICCV2019_SDL-CV" target="_blank" rel="noopener noreferrer">Workshop on Statistical Deep Learning for Computer Vision</a>, ICCV 2019. </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/he2025seeing.mp4" class="pub-video" preload="metadata" muted playsinline autoplay loop onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="he2025seeing" class="col-sm-7"> <div class="title">From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning</div> <div class="author">Honglin He, Yukai Ma,  <em>Wayne Wu</em>, and Bolei Zhou </div> <div class="periodical"> <em>Technical report, arXiv:2507.22028,</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2507.22028" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://metadriverse.github.io/s2e/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/wayne2025urbansim.mp4" class="pub-video" preload="metadata" muted playsinline autoplay loop onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="wayne2025urbansim" class="col-sm-7"> <div class="title">Towards Autonomous Micromobility through Scalable Urban Simulation</div> <div class="author"> <em>Wayne Wu</em> *, Honglin He*, Chaoyuan Zhang, Jack He, Seth Z. Zhao, Ran Gong, Quanyi Li, and Bolei Zhou </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition (CVPR),</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2505.00690" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://metadriverse.github.io/urban-sim/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Highlight</span> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/ziyang2025vid2sim.mp4" class="pub-video" preload="metadata" muted playsinline autoplay loop onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="ziyang2025vid2sim" class="col-sm-7"> <div class="title">Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation</div> <div class="author">Ziyang Xie, Zhizheng Liu, Zhenghao Peng,  <em>Wayne Wu</em>, and Bolei Zhou </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition (CVPR),</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2501.06693" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://metadriverse.github.io/vid2sim/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/wayne2025metaurban.mp4" class="pub-video" preload="metadata" muted playsinline autoplay loop onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="wayne2025metaurban" class="col-sm-7"> <div class="title">MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility</div> <div class="author"> <em>Wayne Wu</em> *, Honglin He*, Jack He, Yiran Wang, Chenda Duan, Zhizheng Liu, Quanyi Li, and Bolei Zhou </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR),</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2407.08725" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=vHuAzNxmfKc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://metadriverse.github.io/metaurban/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Spotlight</span> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/shikai2024cosmicman.mp4" class="pub-video" preload="metadata" muted playsinline autoplay loop onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="shikai2024cosmicman" class="col-sm-7"> <div class="title">CosmicMan: A Text-to-Image Foundation Model for Humans</div> <div class="author">Shikai Li, Jianglin Fu, Kaiyuan Liu, Wentao Wang, Kwan-Yee Lin, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition (CVPR),</em> 2024 </div> <div class="links"> <a href="http://arxiv.org/abs/2404.01294" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=fk2fniU6oyM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://cosmicman-cvpr2024.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Highlight</span> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/contributors2023renderme360.mp4" class="pub-video" preload="metadata" muted playsinline autoplay loop onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="contributors2023renderme360" class="col-sm-7"> <div class="title">RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars</div> <div class="author"> Contributors to RenderMe-360 </div> <div class="periodical"> <em>Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks,</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2305.13353" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=nIgrtQwkrdg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://renderme-360.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/contributors2023dnarendering.mp4" class="pub-video" preload="metadata" muted playsinline autoplay loop onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="contributors2023dnarendering" class="col-sm-7"> <div class="title">DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering</div> <div class="author"> Contributors to DNA-Rendering </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2307.10173" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=xlhfvxvu7nc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://dna-rendering.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/zhitao2023synbody.mp4" class="pub-video" preload="metadata" muted playsinline autoplay loop onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="zhitao2023synbody" class="col-sm-7"> <div class="title">SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling</div> <div class="author">Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai,  <em>Wayne Wu</em>, Chen Qian, Dahua Lin, Ziwei Liu, and Lei Yang </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2303.17368" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=ogXpRB9zR9A" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://synbody.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/tong2023omniobject3d-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/tong2023omniobject3d-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/tong2023omniobject3d-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/tong2023omniobject3d.png"> </picture> </figure> </div> <div id="tong2023omniobject3d" class="col-sm-7"> <div class="title">OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation</div> <div class="author">Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan,  <em>Wayne Wu</em>, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, and Ziwei Liu </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2301.07525" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://omniobject3d.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <span style="color: rgb(255, 60, 0); font-weight: bold; margin-left: 0.5em; display: inline-block; margin-top: 0.5em;">Best Paper Candidate</span> </div> </div> </li> <li> <div class="row"> <div class="col-sm-5"><div class="pub-video-wrapper"> <video src="/assets/video/wayne2018look.mp4" class="pub-video" preload="metadata" muted playsinline autoplay loop onmouseover="this.setAttribute('controls', 'controls')" onmouseout="this.removeAttribute('controls')"></video> </div></div> <div id="wayne2018look" class="col-sm-7"> <div class="title">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</div> <div class="author"> <em>Wayne Wu</em>, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, and Qiang Zhou </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/abs/1805.10483" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=B5IIOYlL4w0&amp;feature=emb_imp_woyt" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/LAB/LAB.html" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Wayne Wu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: October 04, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-4GRHT7MFBP"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-4GRHT7MFBP");</script> </body> </html>