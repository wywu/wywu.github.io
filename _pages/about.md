---
layout: about
title: About
permalink: /
# subtitle-1: <b> Associate Director of R&D @ XR-Lab & SmartVideo Group, <a href='https://www.sensetime.com/en'>SenseTime Group Inc.</a></b>
# subtitle-2: <b> Adjunct Research Scientist @ XR-Research Group, <a href='https://www.shlab.org.cn/'>Shanghai AI Lab.</a></b>

profile:
  align: right
  image: wayne_wu.jpg

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
---

I am a Research Associate in the Department of Computer Science at the University of California, Los Angeles, working with [Bolei Zhou](https://boleizhou.github.io/).
Previously, I served as a Research Scientist at Shanghai AI Lab, where I led the Virtual Human Group.
I was also a Visiting Scholar at Nanyang Technological University, working with [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/).
In June 2022, I obtained my PhD in the Department of Computer Science and Technology at Tsinghua University.

<div class="research">
  <h2>Research</h2>

  <p>My research lies at the intersection of computer vision, computer graphics, and robotics. My goal is to develop <strong>Human-centric Embodied AI Systems</strong> that promote urban mobility, safety, and accessibility, by addressing key challenges in <em>scalability</em> of agent learning environments, as well as <em>realism</em> and <em>situationality</em> of populated virtual humans. This effort is pursued through three primary directions:</p>

  <ul>
    <li><strong>Scalable Embodied AI Simulators</strong>: Building <u>large-scale robot learning platforms</u> with diverse assets and infinite urban scenes, and enabling high-efficiency training, as in <a href="https://metadriverse.github.io/metaurban/">MetaUrban</a>, <a href="publication/">UrbanSim</a>, <a href="https://metadriverse.github.io/vid2sim/">Vid2Sim</a>, and <a href="https://omniobject3d.github.io/">OmniObject3D</a>.</li>
    <li><strong>Realistic Virtual Humans</strong>: Constructing <u>high-fidelity 4D volumetric capture systems and datasets</u>, as in <a href="https://dna-rendering.github.io/">DNA-Rendering</a> and <a href="https://renderme-360.github.io/">RenderMe-360</a>; and developing <u>human foundation models</u> to obtain generalizable representations, as in <a href="https://cosmicman-cvpr2024.github.io/">CosmicMan</a> and <a href="https://motionbert.github.io/">MotionBert</a>.</li>
    <li><strong>Situational Motion Generation</strong>: Learning <u>context-aware generative models</u> of human motions and behaviors from diverse modalities -- including vision, audio, and language, as in <a href="https://genforce.github.io/PedGen/">PedGen</a>, <a href="https://embodiedhuman.github.io/">EmbodiedHuman</a>, and <a href="https://wywu.github.io/projects/EBT/EBT.html">Everybody's Talkin'</a>.</li>
  </ul>
</div>
