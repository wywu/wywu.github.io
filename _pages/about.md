---
layout: about
title: About
permalink: /
# subtitle-1: <b> Associate Director of R&D @ XR-Lab & SmartVideo Group, <a href='https://www.sensetime.com/en'>SenseTime Group Inc.</a></b>
# subtitle-2: <b> Adjunct Research Scientist @ XR-Research Group, <a href='https://www.shlab.org.cn/'>Shanghai AI Lab.</a></b>

profile:
  align: right
  image: wayne_wu.jpg

news: true  # includes a list of news items
# talks: true  # includes a list of talks
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
---

I am a Research Associate in the Department of Computer Science at the University of California, Los Angeles, working with [Bolei Zhou](https://boleizhou.github.io/).
Previously, I served as a Research Scientist at Shanghai AI Lab, where I led the Virtual Human Group, working with [Dahua Lin](http://dahua.site/).
I was also a Visiting Scholar at Nanyang Technological University, working with [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/).
In June 2022, I obtained my PhD in the Department of Computer Science and Technology at Tsinghua University.

<div class="research-slogan">
<em>"I build world simulators and models for physical AI; I put people first, always."</em>
</div>

<div class="research">
  <h2>Research</h2>

  <p>My research lies at the intersection of computer vision, computer graphics, and robotics. I aim to develop <strong style="color: rgb(255, 60, 0);">human-centric physical AI systems</strong> capable of perceiving, understanding, and interacting with real-world environments populated by humans. To advance this vision, I address key challenges in the <em>scalability</em> of robot learning environments, the <em>situational awareness</em> of agents, and the <em>realism</em> of populated virtual humans. My work explores three primary directions:</p>

  <ul>
    <li><strong>Scalable World Simulators</strong>: Developing <u>large-scale robot learning platforms</u> with diverse assets and infinite urban scenes, and enabling high-efficiency robot training, as in <a href="https://metadriverse.github.io/metaurban/">MetaUrban</a>, <a href="publication/">URBAN-SIM</a>, <a href="https://metadriverse.github.io/vid2sim/">Vid2Sim</a>, and <a href="https://omniobject3d.github.io/">OmniObject3D</a>.</li>
    <li><strong>Situational Behavior Modeling</strong>: Building <u>autonomous decision-making models</u> of humans and other agents, to behave robustly based on multimodal understanding of surroundings -- including vision, audio, and language, as in <a href="https://embodiedhuman.github.io/">EmbodiedHuman</a>, <a href="https://genforce.github.io/PedGen/">PedGen</a>, and <a href="https://metadriverse.github.io/s2e/">Seeing-to-Experiencing</a>.</li>
    <li><strong>Realistic Virtual Humans</strong>: Constructing <u>high-fidelity 4D volumetric capture systems and datasets</u>, as in <a href="https://dna-rendering.github.io/">DNA-Rendering</a> and <a href="https://renderme-360.github.io/">RenderMe-360</a>; and developing <u>human foundation models</u> to obtain generalizable representations, as in <a href="https://cosmicman-cvpr2024.github.io/">CosmicMan</a> and <a href="https://motionbert.github.io/">MotionBert</a>.</li>
  </ul>
</div>

<style>
.research-slogan {
  text-align: center;
  font-size: 1.3em;
  margin: 1.5em 0;
  color: rgb(255, 60, 0) !important;
  font-weight: bold !important;
}

.research-slogan em {
  color: rgb(255, 60, 0) !important;
  font-weight: bold !important;
}
</style>
